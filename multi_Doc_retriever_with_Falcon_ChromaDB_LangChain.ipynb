{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNzmwF65Lo23ee+Pc8U9PQf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/obinnachike/multi-document-retriever-with-Falcon/blob/main/multi_Doc_retriever_with_Falcon_ChromaDB_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKy6gy4eu0Y7"
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers\n",
        "!pip -q install langchain tiktoken chromadb pypdf transformers\n",
        "!pip -q install accelerate bitsandbytes sentencepiece Xformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "id": "VT2xTRC5wZDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import DirectoryLoader"
      ],
      "metadata": {
        "id": "D1eIsf3vwKh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Shafi2016/Youtube/raw/main/stock_market_june_2023.zip -O stock_market_june_2023.zip\n",
        "!unzip stock_market_june_2023.zip"
      ],
      "metadata": {
        "id": "LBVmU_kfwUu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializes a DirectoryLoader to load text files from the specified directory.\n",
        "loader = DirectoryLoader('./stock_market_june_2023/', glob=\"./*.txt\", loader_cls=TextLoader)\n",
        "\n",
        "# Loads all the documents present in the directory.\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "U1OJhrAxwphI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "id": "wp3GukhBw0xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializes a RecursiveCharacterTextSplitter with a specified chunk size and overlap.\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "\n",
        "# Splits the loaded documents into chunks of text using the defined text_splitter.\n",
        "texts = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "887bRjmEw3Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "id": "iSBObm5ow6jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[2]"
      ],
      "metadata": {
        "id": "o7SXn8b3w8fM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "LYF_b60Cw-wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_pipe = AutoModelForCausalLM.from_pretrained(\n",
        "    model,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_pipe,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "YbDgfS7p3FIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "Z1u3dwBtxCeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline(pipeline=pipeline)\n",
        "\n",
        "model_name = \"intfloat/e5-large-v2\"\n",
        "hf = HuggingFaceEmbeddings(model_name=model_name)"
      ],
      "metadata": {
        "id": "k7pY9oE80xBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the DB"
      ],
      "metadata": {
        "id": "DaXVhSip02tY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sets the directory where the embeddings will be stored.\n",
        "persist_directory = 'db'\n",
        "\n",
        "# Sets the HuggingFaceEmbeddings object as the embedding to use.\n",
        "embedding = hf\n",
        "\n",
        "# Uses the Chroma module to convert the texts into embeddings using the specified HuggingFace embeddings.\n",
        "# The resulting embeddings are stored in the persist_directory.\n",
        "vectordb = Chroma.from_documents(documents=texts,\n",
        "                                 embedding=hf,\n",
        "                                 persist_directory=persist_directory)"
      ],
      "metadata": {
        "id": "2mAfCIfq0zt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Persists the generated embeddings database to disk.\n",
        "vectordb.persist()\n",
        "\n",
        "# Releases the memory held by the vectordb object by setting it to None.\n",
        "vectordb = None\n",
        "\n",
        " #Re-initializes the Chroma object from the persisted directory with the specified HuggingFace embeddings.\n",
        "vectordb = Chroma(persist_directory=persist_directory,\n",
        "                  embedding_function=hf)"
      ],
      "metadata": {
        "id": "hnfqJzWS1MUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initializes a retriever object from the vectordb with a specified number of search results (k=3).\n",
        "# The vectordb.as_retriever() function is likely converting the Chroma object, which contains vectorized representations of text, into a retriever object.\n",
        "# The retriever can be used to find the most similar vectors in the database given a query vector.\n",
        "# The search_kwargs={\"k\": 3} argument suggests that the retriever will return the top 3 most similar vectors for each query.\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# create the chain to answer questions\n",
        "# Initializes a RetrievalQA object with the specified HuggingFacePipeline (llm), retriever, and chain type.\n",
        "# The return_source_documents parameter set to True means the original source documents will be included in the returned results.\n",
        "qa_chain = RetrievalQA.from_chain_type(llm= llm,\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)\n",
        "\n",
        "\n",
        " # This function processes the response from a Language Model (llm) and prints the result and sources.\n",
        "def process_llm_response(llm_response):\n",
        "    # Prints the 'result' field from the response, which likely contains the answer from the language model.\n",
        "    print(llm_response['result'])\n",
        "\n",
        "    print('\\n\\nSources:')\n",
        "    # Iterates over the 'source_documents' field in the response, which contains the documents where the answer was found.\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        # Prints the 'source' metadata from each document.\n",
        "        print(source.metadata['source'])"
      ],
      "metadata": {
        "id": "34TbhUN-1WEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Could you please enumerate the companies that have been highlighted for their potential stock growth\"\n",
        "\n",
        "llm_response = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "id": "UOuD7ft81hOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5vc_z6Jn1lZj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}